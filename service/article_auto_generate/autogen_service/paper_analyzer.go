// Package autogen_service æä¾›è®ºæ–‡è‡ªåŠ¨åˆ†ææœåŠ¡
//
// **é‡è¦æç¤º**: æœ¬æ–‡ä»¶ä¸­çš„åˆ†æå‡½æ•°å·²è¢«æ–°çš„æ‰¹æ¬¡è¯„åˆ†ç³»ç»Ÿå–ä»£
// æ¨èä½¿ç”¨: service/batch_scoring_service è¿›è¡Œè®ºæ–‡è¯„åˆ†å’Œåˆ†æ
// å½“å‰æ–‡ä»¶ä»…ä¿ç•™ç¼“å­˜ç®¡ç†åŠŸèƒ½ï¼Œåˆ†æåŠŸèƒ½æ ‡è®°ä¸º @deprecated
package autogen_service

import (
	"encoding/json"
	"fmt"
	"sort"
	"strings"
	"time"

	"blogX_server/global"
	"blogX_server/service/ai_service"
	"blogX_server/service/article_auto_generate/crawler_service"
	"blogX_server/service/common_utils"

	"github.com/sirupsen/logrus"
)

const (
	// AIåˆ†æç»“æœRedisç¼“å­˜é”®å‰ç¼€
	AnalysisResultPrefix = "ai_analysis:"
	// åˆ†æç»“æœç¼“å­˜æ—¶é—´ï¼ˆ7å¤©ï¼‰
	AnalysisCacheExpiration = 23 * time.Hour
)

// PaperAnalysisResult AIåˆ†æç»“æœç»“æ„ä½“
type PaperAnalysisResult struct {
	ArxivID          string   `json:"arxivId"`          // åŸå§‹ArXiv ID
	Title            string   `json:"title"`            // è®ºæ–‡æ ‡é¢˜
	Authors          string   `json:"authors"`          // ä½œè€…
	PublishedDate    string   `json:"publishedDate"`    // å‘è¡¨æ—¶é—´
	Abstract         string   `json:"abstract"`         // AIç”Ÿæˆçš„ä¸­æ–‡æ‘˜è¦
	Score            int      `json:"score"`            // ç§‘ç ”ä»·å€¼è¯„åˆ†(0-100)
	Justification    string   `json:"just"`             // è¯„åˆ†ç†ç”±
	Tags             []string `json:"tags"`             // ä¸»é¢˜æ ‡ç­¾
	AnalyzedAt       string   `json:"analyzedAt"`       // åˆ†ææ—¶é—´
	OriginalAbstract string   `json:"originalAbstract"` // åŸå§‹è‹±æ–‡æ‘˜è¦
	PdfURL           string   `json:"pdfUrl"`           // PDFé“¾æ¥
	HtmlURL          string   `json:"htmlUrl"`          // HTMLé“¾æ¥
}

// AIAnalysisResponse AIè¿”å›çš„åˆ†æç»“æœ
type AIAnalysisResponse struct {
	Abstract string   `json:"abstract"` // ä¸­æ–‡æ‘˜è¦
	Score    int      `json:"score"`    // è¯„åˆ†
	Just     string   `json:"just"`     // è¯„åˆ†ç†ç”±
	Tags     []string `json:"tags"`     // æ ‡ç­¾
}

// AnalyzePaper åˆ†æå•ç¯‡è®ºæ–‡ï¼ˆè‡ªåŠ¨å¤„ç†ç¼“å­˜ï¼‰
// @deprecated æ¨èä½¿ç”¨ batch_scoring_service.TwoStageAnalyzer è¿›è¡Œæ‰¹æ¬¡è¯„åˆ†
func (s *AutogenService) AnalyzePaper(paper *crawler_service.ArxivPaper) (*PaperAnalysisResult, error) {
	if paper == nil {
		return nil, fmt.Errorf("è®ºæ–‡æ•°æ®ä¸ºç©º")
	}

	// 1. é¦–å…ˆæ£€æŸ¥ç¼“å­˜
	if cached := s.getAnalysisFromCache(paper.ArxivID); cached != nil {
		logrus.Debugf("ä½¿ç”¨ç¼“å­˜çš„åˆ†æç»“æœ: %s", paper.ArxivID)
		return cached, nil
	}

	logrus.Infof("å¼€å§‹AIåˆ†æè®ºæ–‡: %s", paper.ArxivID)

	// 2. æ„å»ºAIåˆ†æçš„è¾“å…¥æ–‡æœ¬
	inputText := fmt.Sprintf("æ ‡é¢˜ï¼š%s\nå†…å®¹ï¼š%s", paper.Title, paper.Abstract)

	// 3. è°ƒç”¨AIæœåŠ¡è¿›è¡Œåˆ†æ
	aiResponse, err := ai_service.Autogen(inputText)
	if err != nil {
		logrus.Errorf("AIåˆ†æå¤±è´¥: %v", err)
		return nil, fmt.Errorf("AIåˆ†æå¤±è´¥: %v", err)
	}

	// 4. æ¸…ç†AIè¿”å›çš„ç»“æœï¼Œå»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°
	cleanResponse := strings.TrimSpace(aiResponse)
	cleanResponse = strings.TrimPrefix(cleanResponse, "```json")
	cleanResponse = strings.TrimPrefix(cleanResponse, "```")
	cleanResponse = strings.TrimSuffix(cleanResponse, "```")
	cleanResponse = strings.TrimSpace(cleanResponse)

	// 5. æ¸…ç†æ— æ•ˆçš„JSONè½¬ä¹‰åºåˆ—
	cleanResponse = cleanInvalidJSONEscapes(cleanResponse)

	// 6. è§£æAIè¿”å›çš„JSONç»“æœ
	var aiResult AIAnalysisResponse
	err = json.Unmarshal([]byte(cleanResponse), &aiResult)
	if err != nil {
		logrus.Errorf("AIè¿”å›ç»“æœè§£æå¤±è´¥: %v, åŸå§‹ç»“æœ: %s", err, aiResponse)
		return nil, fmt.Errorf("AIè¿”å›ç»“æœè§£æå¤±è´¥: %v", err)
	}

	// 7. æ„å»ºæœ€ç»ˆç»“æœ
	result := &PaperAnalysisResult{
		ArxivID:          paper.ArxivID,
		Title:            paper.Title,
		Authors:          paper.Authors,
		PublishedDate:    paper.PublishedDate,
		Abstract:         aiResult.Abstract,
		Score:            aiResult.Score,
		Justification:    aiResult.Just,
		Tags:             aiResult.Tags,
		AnalyzedAt:       time.Now().Format("2006-01-02 15:04:05"),
		OriginalAbstract: paper.Abstract,
		PdfURL:           fmt.Sprintf("https://arxiv.org/pdf/%s.pdf", paper.ArxivID),
		HtmlURL:          fmt.Sprintf("https://arxiv.org/abs/%s", paper.ArxivID),
	}

	// 8. è‡ªåŠ¨ä¿å­˜åˆ°ç¼“å­˜
	s.saveAnalysisToCache(result)

	logrus.Infof("è®ºæ–‡ %s åˆ†æå®Œæˆï¼Œè¯„åˆ†: %dï¼ˆå·²ç¼“å­˜ï¼‰", paper.ArxivID, result.Score)
	return result, nil
}

// AnalyzePapers æ‰¹é‡åˆ†æè®ºæ–‡ï¼ˆè‡ªåŠ¨ç¼“å­˜ï¼‰
// @deprecated æ¨èä½¿ç”¨ batch_scoring_service.TwoStageAnalyzer è¿›è¡Œæ‰¹æ¬¡è¯„åˆ†
func (s *AutogenService) AnalyzePapers(papers []crawler_service.ArxivPaper) ([]*PaperAnalysisResult, error) {
	if len(papers) == 0 {
		return nil, fmt.Errorf("è®ºæ–‡åˆ—è¡¨ä¸ºç©º")
	}

	results := make([]*PaperAnalysisResult, 0, len(papers))

	logrus.Infof("å¼€å§‹æ‰¹é‡åˆ†æ %d ç¯‡è®ºæ–‡", len(papers))

	for i, paper := range papers {
		result, err := s.AnalyzePaper(&paper)
		if err != nil {
			logrus.Errorf("åˆ†æç¬¬ %d ç¯‡è®ºæ–‡å¤±è´¥: %v", i+1, err)
			// ç»§ç»­å¤„ç†å…¶ä»–è®ºæ–‡ï¼Œè€Œä¸æ˜¯ä¸­æ–­æ•´ä¸ªè¿‡ç¨‹
			continue
		}

		results = append(results, result)

		// ä¸ºäº†é¿å…APIé™åˆ¶ï¼Œåœ¨æ¯æ¬¡è¯·æ±‚ä¹‹é—´æ·»åŠ çŸ­æš‚å»¶è¿Ÿ
		if i < len(papers)-1 {
			time.Sleep(1 * time.Second)
		}
	}

	logrus.Infof("æ‰¹é‡åˆ†æå®Œæˆï¼ŒæˆåŠŸåˆ†æ %d/%d ç¯‡è®ºæ–‡", len(results), len(papers))
	return results, nil
}

// AnalyzePapersWithCache å¸¦ç¼“å­˜çš„æ‰¹é‡è®ºæ–‡åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
// @deprecated æ¨èä½¿ç”¨ batch_scoring_service.TwoStageAnalyzer è¿›è¡Œæ‰¹æ¬¡è¯„åˆ†
func (s *AutogenService) AnalyzePapersWithCache(papers []crawler_service.ArxivPaper) ([]*PaperAnalysisResult, error) {
	logrus.Infof("å¼€å§‹åˆ†æ %d ç¯‡è®ºæ–‡ï¼ˆè‡ªåŠ¨ç¼“å­˜ï¼‰", len(papers))

	var results []*PaperAnalysisResult

	// ç›´æ¥è°ƒç”¨AnalyzePaperï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†ç¼“å­˜
	for _, paper := range papers {
		result, err := s.AnalyzePaper(&paper)
		if err != nil {
			logrus.Errorf("åˆ†æè®ºæ–‡å¤±è´¥ %s: %v", paper.ArxivID, err)
			continue // è·³è¿‡å¤±è´¥çš„è®ºæ–‡ï¼Œç»§ç»­åˆ†æå…¶ä»–è®ºæ–‡
		}
		results = append(results, result)
	}

	logrus.Infof("åˆ†æå®Œæˆï¼Œæ€»å…± %d ç¯‡è®ºæ–‡", len(results))
	return results, nil
}

// GetTopScoredPapers è·å–è¯„åˆ†æœ€é«˜çš„è®ºæ–‡
func GetTopScoredPapers(results []*PaperAnalysisResult, topN int) []*PaperAnalysisResult {
	// ç®€å•çš„æ’åºï¼ŒæŒ‰è¯„åˆ†é™åº
	sort.Slice(results, func(i, j int) bool {
		return results[i].Score > results[j].Score
	})

	if topN <= 0 {
		return results
	}

	// ç¡®ä¿topNä¸è¶…è¿‡æ•°ç»„é•¿åº¦
	if topN > len(results) {
		topN = len(results)
	}

	return results[:topN]
}

// FormatAnalysisReport æ ¼å¼åŒ–åˆ†ææŠ¥å‘Šä¸º Markdown æ ¼å¼
// @deprecated æ¨èä½¿ç”¨ article_autogen.formatTwoStageAnalysisReport
func FormatAnalysisReport(results []*PaperAnalysisResult, category string) string {
	if len(results) == 0 {
		return "## ğŸ“Š AIè®ºæ–‡åˆ†ææŠ¥å‘Š\n\n> âŒ æ²¡æœ‰åˆ†æç»“æœ"
	}

	var body strings.Builder

	// æŠ¥å‘Šå¤´éƒ¨
	body.WriteString("### æ¯å¤©è®© AI åŠ©æ‰‹ï¼Œä»ç‰¹å®šé¢†åŸŸ ç™¾ç¯‡æœ€æ–°è®ºæ–‡ ä¸­æ•´ç†æŒ‘é€‰æœ€å¥½çš„ 30 ç¯‡ï¼Œä¾›æ‚¨é˜…è¯»\n\n")
	body.WriteString("### ä»Šæ—¥æ¦‚è§ˆ\n\n")
	body.WriteString(fmt.Sprintf("**å­¦æœ¯é¢†åŸŸ**: %s  \n", category))
	body.WriteString(fmt.Sprintf("**ç”Ÿæˆæ—¶é—´**: %s  \n", time.Now().Format("2006-01-02 15:04:05")))
	body.WriteString(fmt.Sprintf("**åˆ†ææ•°é‡**: %d ç¯‡è®ºæ–‡  \n", len(results)))

	// è¯„åˆ†æ¦‚è§ˆ
	var scores []int
	for _, result := range results {
		scores = append(scores, result.Score)
	}
	avgScore := common_utils.CalculateAverage(scores)
	maxScore := common_utils.FindMax(scores)
	minScore := common_utils.FindMin(scores)

	body.WriteString(fmt.Sprintf("- **æœ€é«˜åˆ†**: `%d`  \n", maxScore))
	body.WriteString(fmt.Sprintf("- **å¹³å‡åˆ†**: `%.1f`  \n", avgScore))
	body.WriteString(fmt.Sprintf("- **æœ€ä½åˆ†**: `%d`  \n\n", minScore))

	body.WriteString("---\n\n")

	// ğŸ“ è®ºæ–‡è¯¦æƒ…
	for i, paper := range results {
		// ğŸ‘¥ ä½œè€…åç§°æˆªæ–­å¤„ç†
		authors := common_utils.TruncateAuthor(paper.Authors, 100)

		body.WriteString(fmt.Sprintf("### %02d %s\n\n", i, paper.Title))
		body.WriteString(fmt.Sprintf("**ä½œè€…**: %s  \n", authors))
		// ğŸ“‹ æ ‡ç­¾å±•ç¤º
		if len(paper.Tags) > 0 {
			body.WriteString("**æ ‡ç­¾**: ")
			for j, tag := range paper.Tags {
				body.WriteString(fmt.Sprintf("`%s`", tag))
				if j < len(paper.Tags)-1 {
					body.WriteString(" ")
				}
			}
			body.WriteString("  \n")
		}
		body.WriteString(fmt.Sprintf("**åˆ†ææ—¶é—´**: %s  \n", paper.AnalyzedAt))
		body.WriteString(fmt.Sprintf("**è®ºæ–‡æº**: [ ArXiv ](%s) | [ PDF ](%s)  \n", paper.HtmlURL, paper.PdfURL))
		body.WriteString(fmt.Sprintf("**AIæ‘˜è¦**: %s\n\n", paper.Abstract))
		body.WriteString(fmt.Sprintf("**æœ¬ç«™è¯„åˆ†**: `%d/100`\n", paper.Score))
		body.WriteString(fmt.Sprintf("**è¯„åˆ†åˆ†æ**: %s  \n", paper.Justification))

		// åˆ†éš”ç¬¦ï¼ˆæœ€åä¸€ç¯‡ä¸æ·»åŠ ï¼‰
		if i < len(results)-1 {
			body.WriteString("---\n\n")
		}
	}

	return body.String()
}

// AnalyzePapersForWriting ä¸“é—¨ç”¨äºå†™æ–‡ç« çš„è®ºæ–‡åˆ†æï¼ˆæ¨èä½¿ç”¨ï¼‰
// @deprecated æ¨èä½¿ç”¨ batch_scoring_service.TwoStageAnalyzer è¿›è¡Œæ‰¹æ¬¡è¯„åˆ†
func (s *AutogenService) AnalyzePapersForWriting(category crawler_service.ArxivCategory, limit int, topN int) ([]*PaperAnalysisResult, error) {
	// 1. å®æ—¶çˆ¬å–æœ€æ–°è®ºæ–‡
	crawler := crawler_service.NewArxivCrawlerWithCategory(category)
	papers, err := crawler.CrawlRecentPapers()
	if err != nil {
		return nil, fmt.Errorf("çˆ¬å–è®ºæ–‡å¤±è´¥: %v", err)
	}

	// é™åˆ¶æ•°é‡
	if limit > 0 && len(papers) > limit {
		papers = papers[:limit]
	}

	// 2. æ‰¹é‡åˆ†æï¼ˆå¸¦ç¼“å­˜ï¼‰
	results, err := s.AnalyzePapersWithCache(papers)
	if err != nil {
		return nil, err
	}

	// 3. æŒ‰è¯„åˆ†æ’åºï¼Œè¿”å›Top N
	topPapers := GetTopScoredPapers(results, topN)

	logrus.Infof("è®ºæ–‡åˆ†æå®Œæˆï¼Œä» %d ç¯‡ä¸­é€‰å‡º %d ç¯‡é«˜åˆ†è®ºæ–‡ç”¨äºå†™ä½œ", len(papers), len(topPapers))
	return topPapers, nil
}

// AnalyzePapersFromList ä»ç»™å®šè®ºæ–‡åˆ—è¡¨ä¸­åˆ†æå¹¶é€‰æ‹©é«˜åˆ†è®ºæ–‡
// @deprecated æ¨èä½¿ç”¨ batch_scoring_service.TwoStageAnalyzer è¿›è¡Œæ‰¹æ¬¡è¯„åˆ†
func (s *AutogenService) AnalyzePapersFromList(papers []crawler_service.ArxivPaper, topN int) ([]*PaperAnalysisResult, error) {
	// 1. æ‰¹é‡åˆ†æï¼ˆä¸ç¼“å­˜ï¼Œç¡®ä¿è¯„åˆ†æ ‡å‡†ä¸€è‡´ï¼‰
	results, err := s.AnalyzePapers(papers)
	if err != nil {
		return nil, err
	}

	// 2. æŒ‰è¯„åˆ†æ’åºï¼Œè¿”å›Top N
	topPapers := GetTopScoredPapers(results, topN)

	logrus.Infof("è®ºæ–‡åˆ†æå®Œæˆï¼Œä» %d ç¯‡ä¸­é€‰å‡º %d ç¯‡é«˜åˆ†è®ºæ–‡ç”¨äºå†™ä½œ", len(papers), len(topPapers))
	return topPapers, nil
}

// getAnalysisFromCache ä»Redisç¼“å­˜è·å–AIåˆ†æç»“æœ
func (s *AutogenService) getAnalysisFromCache(arxivID string) *PaperAnalysisResult {
	if global.Redis == nil {
		return nil
	}

	cacheKey := AnalysisResultPrefix + arxivID
	resultJSON, err := global.Redis.Get(cacheKey).Result()
	if err != nil {
		// ç¼“å­˜æœªå‘½ä¸­ï¼Œæ­£å¸¸æƒ…å†µ
		return nil
	}

	var result PaperAnalysisResult
	err = json.Unmarshal([]byte(resultJSON), &result)
	if err != nil {
		logrus.Errorf("ååºåˆ—åŒ–åˆ†æç»“æœå¤±è´¥ %s: %v", arxivID, err)
		// åˆ é™¤æŸåçš„ç¼“å­˜
		global.Redis.Del(cacheKey)
		return nil
	}

	logrus.Debugf("ä»ç¼“å­˜è·å–åˆ†æç»“æœ: %s", arxivID)
	return &result
}

// saveAnalysisToCache ä¿å­˜AIåˆ†æç»“æœåˆ°Redisç¼“å­˜
func (s *AutogenService) saveAnalysisToCache(result *PaperAnalysisResult) {
	if global.Redis == nil || result == nil {
		return
	}

	cacheKey := AnalysisResultPrefix + result.ArxivID
	resultJSON, err := json.Marshal(result)
	if err != nil {
		logrus.Errorf("åºåˆ—åŒ–åˆ†æç»“æœå¤±è´¥ %s: %v", result.ArxivID, err)
		return
	}

	err = global.Redis.Set(cacheKey, resultJSON, AnalysisCacheExpiration).Err()
	if err != nil {
		logrus.Errorf("ä¿å­˜åˆ†æç»“æœåˆ°ç¼“å­˜å¤±è´¥ %s: %v", result.ArxivID, err)
		return
	}

	logrus.Debugf("ä¿å­˜åˆ†æç»“æœåˆ°ç¼“å­˜: %s (7å¤©è¿‡æœŸ)", result.ArxivID)
}

// ClearAnalysisCache æ¸…ç†åˆ†æç»“æœç¼“å­˜
func (s *AutogenService) ClearAnalysisCache() error {
	if global.Redis == nil {
		return fmt.Errorf("redisæœªè¿æ¥")
	}

	pattern := AnalysisResultPrefix + "*"
	keys, err := global.Redis.Keys(pattern).Result()
	if err != nil {
		return fmt.Errorf("è·å–ç¼“å­˜é”®å¤±è´¥: %v", err)
	}

	if len(keys) == 0 {
		logrus.Info("æ²¡æœ‰éœ€è¦æ¸…ç†çš„åˆ†æç¼“å­˜")
		return nil
	}

	deleted, err := global.Redis.Del(keys...).Result()
	if err != nil {
		return fmt.Errorf("åˆ é™¤ç¼“å­˜å¤±è´¥: %v", err)
	}

	logrus.Infof("æˆåŠŸæ¸…ç† %d ä¸ªåˆ†æç»“æœç¼“å­˜", deleted)
	return nil
}

// GetCacheStats è·å–åˆ†æç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
func (s *AutogenService) GetCacheStats() (map[string]interface{}, error) {
	if global.Redis == nil {
		return nil, fmt.Errorf("redisæœªè¿æ¥")
	}

	pattern := AnalysisResultPrefix + "*"
	keys, err := global.Redis.Keys(pattern).Result()
	if err != nil {
		return nil, fmt.Errorf("è·å–ç¼“å­˜é”®å¤±è´¥: %v", err)
	}

	stats := map[string]interface{}{
		"total_cached":    len(keys),
		"cache_prefix":    AnalysisResultPrefix,
		"expiration_days": int(AnalysisCacheExpiration.Hours() / 24),
	}

	if len(keys) > 0 {
		// è·å–ä¸€ä¸ªæ ·æœ¬çš„TTL
		ttl, err := global.Redis.TTL(keys[0]).Result()
		if err == nil {
			stats["sample_ttl_hours"] = int(ttl.Hours())
		}
	}

	return stats, nil
}

// calculateAverage è®¡ç®—å¹³å‡å€¼
// @deprecated ä½¿ç”¨ common_utils.CalculateAverage æ›¿ä»£
func calculateAverage(scores []int) float64 {
	return common_utils.CalculateAverage(scores)
}

// findMax æ‰¾æœ€å¤§å€¼
// @deprecated ä½¿ç”¨ common_utils.FindMax æ›¿ä»£
func findMax(scores []int) int {
	return common_utils.FindMax(scores)
}

// findMin æ‰¾æœ€å°å€¼
// @deprecated ä½¿ç”¨ common_utils.FindMin æ›¿ä»£
func findMin(scores []int) int {
	return common_utils.FindMin(scores)
}

// getScoreEmoji æ ¹æ®è¯„åˆ†è¿”å›è¡¨æƒ…ç¬¦å·
// @deprecated ä½¿ç”¨ common_utils.GetScoreEmoji æ›¿ä»£
func getScoreEmoji(score int) string {
	return common_utils.GetScoreEmoji(float64(score))
}

// cleanInvalidJSONEscapes æ¸…ç†æ— æ•ˆçš„JSONè½¬ä¹‰åºåˆ—
func cleanInvalidJSONEscapes(jsonStr string) string {
	// å®šä¹‰æœ‰æ•ˆçš„JSONè½¬ä¹‰å­—ç¬¦
	validEscapes := map[string]bool{
		"\\\"": true, // å¼•å·
		"\\\\": true, // åæ–œæ 
		"\\/":  true, // æ–œæ 
		"\\b":  true, // é€€æ ¼
		"\\f":  true, // æ¢é¡µ
		"\\n":  true, // æ¢è¡Œ
		"\\r":  true, // å›è½¦
		"\\t":  true, // åˆ¶è¡¨ç¬¦
	}

	var result strings.Builder
	i := 0

	for i < len(jsonStr) {
		if jsonStr[i] == '\\' && i+1 < len(jsonStr) {
			// æ£€æŸ¥æ˜¯å¦æ˜¯Unicodeè½¬ä¹‰ \uXXXX
			if jsonStr[i+1] == 'u' && i+5 < len(jsonStr) {
				// æ£€æŸ¥åé¢4ä¸ªå­—ç¬¦æ˜¯å¦éƒ½æ˜¯åå…­è¿›åˆ¶
				isValidUnicode := true
				for j := i + 2; j < i+6; j++ {
					c := jsonStr[j]
					if !((c >= '0' && c <= '9') || (c >= 'a' && c <= 'f') || (c >= 'A' && c <= 'F')) {
						isValidUnicode = false
						break
					}
				}

				if isValidUnicode {
					// æœ‰æ•ˆçš„Unicodeè½¬ä¹‰ï¼Œä¿ç•™
					result.WriteString(jsonStr[i : i+6])
					i += 6
					continue
				}
			}

			// æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„2å­—ç¬¦è½¬ä¹‰åºåˆ—
			escapeSeq := jsonStr[i : i+2]
			if validEscapes[escapeSeq] {
				// æœ‰æ•ˆè½¬ä¹‰ï¼Œä¿ç•™
				result.WriteString(escapeSeq)
				i += 2
			} else {
				// æ— æ•ˆè½¬ä¹‰ï¼Œç§»é™¤åæ–œæ 
				logrus.Warnf("æ¸…ç†æ— æ•ˆJSONè½¬ä¹‰åºåˆ—: %s", escapeSeq)
				result.WriteByte(jsonStr[i+1]) // åªä¿ç•™è½¬ä¹‰åçš„å­—ç¬¦
				i += 2
			}
		} else {
			// æ™®é€šå­—ç¬¦ï¼Œç›´æ¥æ·»åŠ 
			result.WriteByte(jsonStr[i])
			i++
		}
	}

	return result.String()
}
